{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a2724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import pandas.io.sql as sqlio\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "conn = psycopg2.connect(\"user=postgres password='1234'\")\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT);\n",
    "cur = conn.cursor()\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1234@localhost/ccgt')\n",
    "results_dir = 'C:\\\\Users\\\\RonyArmon\\\\Projects_Code\\\\Cluster_Activities\\\\results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d96d6",
   "metadata": {},
   "source": [
    "# Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79da23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = sqlio.read_sql_query('SELECT * FROM projects', engine)\n",
    "names = list(projects_df['Activity Name'].unique())\n",
    "tokens_similarity = pd.read_pickle(os.path.join(results_dir,'tokens_similarity.pkl'))\n",
    "lemmas_synonyms = pd.read_excel(os.path.join(results_dir, 'lemmas_synonyms.xlsx'))\n",
    "embedding_model_names = ['glove-twitter-25'] #, 'word2vec-google-news-300']\n",
    "similarity_matrices = {}\n",
    "results_dir = os.path.join(results_dir, 'similarity/embeddings/')\n",
    "for model_name in embedding_model_names:\n",
    "    similarity_matrices[model_name] = pd.read_pickle(os.path.join(results_dir, '{m}.pkl'.format(m=model_name))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "add6f362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MEX.C003'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_ids = list(projects_df['Activity ID'])\n",
    "sample_id = activity_ids[5]\n",
    "sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8d8f8",
   "metadata": {},
   "source": [
    "# Tokens with Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d57eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short name tokens: 7 [('site', 16), ('drainage', 17), ('a&a', 18), ('works', 19), ('&', 20), ('site', 21), ('demob', 22)]\n",
      "long name tokens: 7 [('turbine', 23), ('hall', 24), ('(', 25), ('pre-bore', 26), ('socket', 27), ('h-piles', 28), (')', 29)]\n"
     ]
    }
   ],
   "source": [
    "id1, id2 = 3,4\n",
    "df1 = pos_df[pos_df['name_id']==id1]\n",
    "df2 = pos_df[pos_df['name_id']==id2]\n",
    "tokens_ids1 = list(zip(df1['token'], df1['token_id']))\n",
    "tokens_ids2 = list(zip(df2['token'], df2['token_id']))\n",
    "if len(tokens_ids1)>len(tokens_ids2):\n",
    "    tokens_ids1_holder = tokens_ids1\n",
    "    tokens_ids1 = tokens_ids2\n",
    "    tokens_ids2 = tokens_ids1_holder\n",
    "print('short name tokens:', len(tokens_ids1), tokens_ids1)\n",
    "print('long name tokens:', len(tokens_ids2), tokens_ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61558db5",
   "metadata": {},
   "source": [
    "# Pairs Scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10265079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['drain']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['knead', 'bring', 'exploit', 'process', 'employment', 'make', 'solve', 'ferment', 'influence', 'crop', 'shape', 'mould', 'forge', 'study', 'lick', 'wreak', 'run', 'form', 'sour', 'turn', 'go', 'cultivate', 'play', 'oeuvre', 'workplace', 'exercise', 'mold', 'function', 'operate', 'act']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: []\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['situation', 'place', 'locate', 'website']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: ['hallway', 'anteroom', 'mansion', 'manse', 'dormitory', 'lobby', 'antechamber', 'dorm', 'foyer', 'residence', 'vestibule']\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "lemma1_syns: ['demobilise', 'demobilize']\n",
      "lemma2_syns: []\n",
      "lemmas_12_syns: set()\n",
      "token_pairs: []\n"
     ]
    }
   ],
   "source": [
    "token_pairs = []\n",
    "for token_id1 in tokens_ids1:\n",
    "    score = 0\n",
    "    token1, id1 = token_id1\n",
    "    checked, scores = [], {}\n",
    "    for token_id2 in tokens_ids2:\n",
    "        token2, id2 = token_id2\n",
    "        if id2 not in checked:\n",
    "            \n",
    "            # String similarity score \n",
    "            score += tokens_similarity.at[token1, token2]\n",
    "            \n",
    "            # Synonym score \n",
    "            lemma1 = pos_df['lemma'][pos_df['token_id']==id1].values[0]\n",
    "            lemma2 = pos_df['lemma'][pos_df['token_id']==id2].values[0]\n",
    "            lemma1_syns = list(lemmas_synonyms['synonym'][lemmas_synonyms['lemma']==lemma1])\n",
    "            lemma2_syns = list(lemmas_synonyms['synonym'][lemmas_synonyms['lemma']==lemma2])\n",
    "            lemmas_12_syns = set(lemma1_syns).intersection(set(lemma2_syns))\n",
    "            if len(lemmas_12_syns)>1: score += 1\n",
    "                        \n",
    "            \n",
    "            # Read and score by embeddings similarity \n",
    "            \n",
    "            checked.append(id2)\n",
    "            #print(token1, id1, token2,id2, score)\n",
    "            # Filter score\n",
    "            if score>0.3: \n",
    "                scores[(token2, id2)]=score\n",
    "    scores_array = np.array(list(scores.values()))\n",
    "    if np.sum(scores_array)>0:\n",
    "        # Filter score\n",
    "        max_score = np.max(scores_array) \n",
    "        match_score = {k:v for k,v in scores.items() if v == max_score}\n",
    "        ms_keys, ms_vals = list(match_score.keys()), list(match_score.values())\n",
    "        match_score = [id1, ms_keys[0][1], ms_vals[0]]\n",
    "    else:\n",
    "        match_score = ()\n",
    "    token_pairs.append(match_score)\n",
    "    #print('match score:', match_score)\n",
    "token_pairs = [t for t in token_pairs if t]\n",
    "print('token_pairs:', token_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f686cc",
   "metadata": {},
   "source": [
    "# Part of Speech Scores (LOCpi, POSpi)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cb12166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "[16, 27, 0.4, 1]\n",
      "upos: NOUN NOUN\n",
      "[16, 27, 0.4, 1, 1]\n",
      "------\n",
      "[17, 23, 0.53, 1]\n",
      "upos: NOUN NOUN\n",
      "[17, 23, 0.53, 1, 1]\n",
      "------\n",
      "[19, 27, 0.36, 1]\n",
      "upos: NOUN NOUN\n",
      "[19, 27, 0.36, 1, 1]\n",
      "------\n",
      "[21, 27, 0.4, 1]\n",
      "upos: NOUN NOUN\n",
      "[21, 27, 0.4, 1, 1]\n",
      "------\n",
      "[22, 26, 0.31, 0]\n",
      "upos: PUNCT ADJ\n",
      "[22, 26, 0.31, 0, 0]\n",
      "[[16, 27, 0.4, 1, 1], [17, 23, 0.53, 1, 1], [19, 27, 0.36, 1, 1], [21, 27, 0.4, 1, 1], [22, 26, 0.31, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "pos_scores = []\n",
    "for pair in token_pairs:\n",
    "    pos_score = 0\n",
    "    print('------')\n",
    "    print(pair)\n",
    "    id1, id2 = pair[0], pair[1]\n",
    "    pos1 = pos_df['upos'][pos_df['token_id']==id1].values[0]\n",
    "    pos2 = pos_df['upos'][pos_df['token_id']==id2].values[0]\n",
    "    print('upos:', pos1, pos2)\n",
    "    if pos1 == pos2: pos_score = 1\n",
    "    pair.append(pos_score)\n",
    "    print(pair)\n",
    "    pos_scores.append(pair)\n",
    "    pair = []\n",
    "print(pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6a85021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token_id  name_id loc      token      lemma   upos xpos head deprel  \\\n",
      "0         1        1   1  construct  construct   VERB   VB    0   root   \n",
      "0         2        1   2     trials      trial   NOUN  NNS    1    obj   \n",
      "0         3        1   3          &          &  CCONJ   CC    4     cc   \n",
      "0         4        1   4       test       test   NOUN   NN    2   conj   \n",
      "0         5        1   5        for        for    ADP   IN    7   case   \n",
      "\n",
      "  start_char end_char                  feats  \n",
      "0          0        9  Mood=Imp|VerbForm=Fin  \n",
      "0         10       16            Number=Plur  \n",
      "0         17       18                      _  \n",
      "0         19       23            Number=Sing  \n",
      "0         24       27                      _  \n"
     ]
    }
   ],
   "source": [
    "print(pos_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores = []\n",
    "for pair in token_pairs:\n",
    "    pos_score = 0\n",
    "    print('------')\n",
    "    print(pair)\n",
    "    id1, id2 = pair[0], pair[1]\n",
    "    pos1 = pos_df['upos'][pos_df['token_id']==id1].values[0]\n",
    "    pos2 = pos_df['upos'][pos_df['token_id']==id2].values[0]\n",
    "    print('upos:', pos1, pos2)\n",
    "    if pos1 == pos2: pos_score = 1\n",
    "    pair.append(pos_score)\n",
    "    print(pair)\n",
    "    pos_scores.append(pair)\n",
    "    pair = []\n",
    "print(pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc49be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
