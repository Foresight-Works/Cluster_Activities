{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a62ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "results_dir = 'C:/Users/RonyArmon/Foresight Works/Foresight Works (OneDrive) - Documents/DS/Cluster_Activities_docs/results/clusters/AgglomerativeClustering_affinityEuclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be626190",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks=\"=|\\+|_|\\.|:|\\/|\\*|\\'|,|\\?|-|\\(|\\)|{|}|\\[|\\]\"\n",
    "regex = '\\(.+?\\)|\\w+\\d{1,}\\.*\\d*\\w*|\\w+' # Including parenthesis\n",
    "tokenizer = nltk.RegexpTokenizer(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(num_clusters, results_dir):\n",
    "    files = os.listdir(results_dir)\n",
    "    num_clusters = str(num_clusters)\n",
    "    # Select the file containing the num_clusters; if more than one file contains the same num_clusters select the first\n",
    "    file = [f for f in files if num_clusters in f][0]\n",
    "    clusters_df = pd.read_excel(os.path.join(results_dir, file))\n",
    "    clusters = clusters_df['cluster'].unique()\n",
    "    clusters_names = {}\n",
    "    for cluster in clusters:\n",
    "        names = list(clusters_df['name'][clusters_df['cluster']==cluster].values)\n",
    "        clusters_names[cluster]=names\n",
    "    \n",
    "    return clusters_names\n",
    "\n",
    "def names_to_parts(names):\n",
    "    '''\n",
    "    Split a list of names by hyphens and store each part in a dedicated list\n",
    "    '''\n",
    "    tokens = []\n",
    "    parts1, parts2, parts3 = [], [], []     \n",
    "    for name in cluster_names:\n",
    "        # Filter tokens in parenthesis\n",
    "        name_split = name.split('-')\n",
    "        num_splits = len(name_split) \n",
    "        if num_splits >= 1:\n",
    "            parts1.append(name_split[0])\n",
    "            if num_splits >= 2:\n",
    "                parts2.append(name_split[1])\n",
    "                if num_splits > 2:\n",
    "                    parts3.append(name_split[2])\n",
    "    \n",
    "    names_parts = [parts1, parts2, parts3]\n",
    "    names_parts = [p for p in names_parts if p]\n",
    "    return names_parts\n",
    "\n",
    "def get_names_matches(texts_list):\n",
    "    match_parts = []\n",
    "    for n1 in texts_list:\n",
    "        for n2 in texts_list:\n",
    "            #print('-------')\n",
    "            #print(n1)\n",
    "            #print(n2)\n",
    "            matches = SequenceMatcher(None, n1, n2).get_matching_blocks()\n",
    "            for match in matches:\n",
    "                a,b,s = match\n",
    "                ##print(a,b,s)\n",
    "                match = n1[a:a+s]\n",
    "                if s>2: \n",
    "                    #print('match:')\n",
    "                    #print(match)\n",
    "                    match_parts.append(match)\n",
    "    match_parts = [re.sub('\\s{2,}', ' ', n).rstrip().lstrip() for n in match_parts]\n",
    "    match_parts_counts = dict(Counter(match_parts))\n",
    "    vals = list(match_parts_counts.values())\n",
    "    vals.sort(reverse=True)\n",
    "    frequent_part = vals[0]\n",
    "    frequent_parts = {k for k,v in match_parts_counts.items() if v==frequent_part}\n",
    "    return frequent_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4b1b9bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_names\n",
      " ['design criteria - instrumentation and controls - final', 'st i&c transport to site field instrumentation', 'instrumentation', 'pulling data preparation on site', 'fo tank  i&c checks on tank & off loading station instrumentation', 'design criteria - instrumentation and controls - preliminary', 'install speed measuring facilities', 'site establishment - install instrumentation to monitor building settlment, ground/utility settlement and vibrations']\n",
      "names parts: [['design criteria ', 'st i&c transport to site field instrumentation', 'instrumentation', 'pulling data preparation on site', 'fo tank  i&c checks on tank & off loading station instrumentation', 'design criteria ', 'install speed measuring facilities', 'site establishment '], [' instrumentation and controls ', ' instrumentation and controls ', ' install instrumentation to monitor building settlment, ground/utility settlement and vibrations'], [' final', ' preliminary']]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline (Single cluster) \n",
    "clusters_names = get_clusters(400, results_dir)\n",
    "cluster_names = clusters_names[398]\n",
    "# Exclude tokens in parenthesis\n",
    "cluster_names = [re.sub('\\(.+?\\)|\\[.+?\\]','',n).rstrip().lstrip() for n in cluster_names]\n",
    "print('cluster_names\\n', cluster_names)\n",
    "parts = names_to_parts(cluster_names)\n",
    "print('names parts:', parts)\n",
    "frequent_parts = []\n",
    "for part in parts: frequent_parts.append(get_names_matches(part))\n",
    "print(frequent_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0628da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84ee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11ba04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4674e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(match_parts_counts.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(match_parts_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_matches = [n for n in names_matches if len(n.split(' '))>1]\n",
    "names_matches = [re.sub(punctuation_marks,'',n).rstrip().lstrip() for n in names_matches]\n",
    "names_matches = [n for n in names_matches if len(n.split(' '))>1]\n",
    "names_matches = [re.sub('\\s{2,}', ' ', n) for n in names_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf139f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_str = ''\n",
    "for n in parts2: \n",
    "    n = n + ' '\n",
    "    part_str += n\n",
    "part_str = part_str.lstrip().rstrip()\n",
    "part_str = re.sub('\\s{2,}', ' ', part_str)\n",
    "tokens = tokenizer.tokenize(part_str)\n",
    "parenthesis_tokens = [t for t in tokens if re.findall('\\(.+?\\)', t)]\n",
    "tokens = [t for t in tokens if t not in parenthesis_tokens]\n",
    "counts = Counter(tokens)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b507d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                \n",
    "    \n",
    "    for name_part in name_parts:\n",
    "        \n",
    "    name_tokens = nltk.word_tokenize(name)\n",
    "    tokens += name_tokens\n",
    "\n",
    "\n",
    "print('{} tokens'.format(len(tokens)))\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61734d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'S624 Excavate 92m (SZ) (c.rock/total 1864m)'\n",
    "pattern = '\\(.+?\\)|'\n",
    "tokenizer = nltk.RegexpTokenizer(pattern)\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66badd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'S624 Excavate 92m (SZ) (c.rock/total 1864m)'\n",
    "pattern = '\\(.+?\\)|\\w+\\d{1,}\\.*\\d*\\w*|\\w+'\n",
    "tokenizer = nltk.RegexpTokenizer(pattern)\n",
    "print(s)\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67934538",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['apple','egg','apple','banana','egg','apple']\n",
    "counts = Counter(list1)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2da2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
