{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f0847c",
   "metadata": {},
   "source": [
    "Explore the pos results obtained using various packages  \n",
    "Data: The pos results using stanza on the projects' names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94b794a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "def isint(value):\n",
    "    '''\n",
    "    Check if the input value type is integer\n",
    "    '''\n",
    "    try:\n",
    "        int(value)\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a95d0b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159626 rows\n",
      "   token_id  name_id loc      token      lemma   upos xpos head deprel  \\\n",
      "0         1        1   1  construct  construct   VERB   VB    0   root   \n",
      "0         2        1   2     trials      trial   NOUN  NNS    1    obj   \n",
      "0         3        1   3          &          &  CCONJ   CC    4     cc   \n",
      "0         4        1   4       test       test   NOUN   NN    2   conj   \n",
      "0         5        1   5        for        for    ADP   IN    7   case   \n",
      "\n",
      "  start_char end_char                  feats  \n",
      "0          0        9  Mood=Imp|VerbForm=Fin  \n",
      "0         10       16            Number=Plur  \n",
      "0         17       18                      _  \n",
      "0         19       23            Number=Sing  \n",
      "0         24       27                      _  \n"
     ]
    }
   ],
   "source": [
    "results_dir = 'C:\\\\Users\\\\RonyArmon\\\\Projects_Code\\\\Cluster_Activities\\\\results'\n",
    "pos_df = pd.read_pickle(os.path.join(results_dir,'pos.pkl'))\n",
    "print('{} rows'.format(len(pos_df)))\n",
    "print(pos_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55eb285",
   "metadata": {},
   "source": [
    "# Stems and Lemmas  \n",
    "Compare stanza and nltk lemmatizers and stemmers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ecbab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       token StanzaLemmatizer   upos WordNetLemmatizer PorterStemmer  \\\n",
      "0  construct        construct   VERB         construct     construct   \n",
      "1     trials            trial   NOUN             trial         trial   \n",
      "2          &                &  CCONJ                 &             &   \n",
      "3       test             test   NOUN              test          test   \n",
      "4        for              for    ADP               for           for   \n",
      "\n",
      "  LancasterStemmer  \n",
      "0        construct  \n",
      "1              tri  \n",
      "2                &  \n",
      "3             test  \n",
      "4              for  \n"
     ]
    }
   ],
   "source": [
    "stems_lemmas = pos_df[['token', 'lemma', 'upos']].drop_duplicates()\\\n",
    "    .rename(columns={'lemma': 'StanzaLemmatizer'})\n",
    "tokens = list(stems_lemmas['token'])\n",
    "stems_lemmas['WordNetLemmatizer'] = [wnl.lemmatize(t) for t in tokens]\n",
    "stems_lemmas['PorterStemmer'] = [porter.stem(t) for t in tokens]\n",
    "stems_lemmas['LancasterStemmer'] = [lancaster.stem(t) for t in tokens]\n",
    "stems_lemmas = stems_lemmas.reset_index(drop=True)\n",
    "print(stems_lemmas.head())\n",
    "stems_lemmas.to_excel(os.path.join(results_dir,'stems_lemmas.xlsx'), index=False)\n",
    "#stems_lemmas = pd.read_excel('./results/stems_lemmas.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7c6c3",
   "metadata": {},
   "source": [
    "# Progressive forms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf1fe679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            token StanzaLemmatizer PorterStemmer\n",
      "2050      probing          probing         probe\n",
      "3047  engineering      engineering         engin\n",
      "5146    lightning        lightning        lightn\n",
      "5213      tracing          tracing         trace\n",
      "5462      fencing          fencing          fenc\n",
      "116 verbs lemmatized, 6 verbs not lemmatized\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "lemmatized, not_lemmatized = [], []\n",
    "stems_lemmas = stems_lemmas[stems_lemmas['upos']=='VERB']\n",
    "for index, row in stems_lemmas.iterrows():\n",
    "    token, lemma = str(row['token']), str(row['StanzaLemmatizer'])\n",
    "    if re.findall('ing$', token):\n",
    "        if token == lemma:\n",
    "            not_lemmatized.append(token)\n",
    "            indices.append(index)\n",
    "        else:\n",
    "            lemmatized.append(token)            \n",
    "progressive_forms = stems_lemmas[stems_lemmas.index.isin(indices)]\n",
    "progressive_forms = progressive_forms.drop(['upos', 'WordNetLemmatizer', 'LancasterStemmer'], axis=1)\n",
    "print(progressive_forms.head())\n",
    "print('{} verbs lemmatized, {} verbs not lemmatized'.format(len(lemmatized), len(not_lemmatized)))\n",
    "progressive_forms.to_excel(os.path.join(results_dir,'progressive_forms.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d24005",
   "metadata": {},
   "source": [
    "# Synonyms by Wordnet synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07307d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "lemmas = list(set(pos_df['lemma']))\n",
    "synonyms_tokens = {}\n",
    "for token in lemmas:\n",
    "    # Exclude numeric tokens\n",
    "    if type(isint(token))!=int:\n",
    "        # Exclude single character tokens\n",
    "        if len(token)>1:\n",
    "            # Exclude 2nd 3rd 4th\n",
    "            if not re.findall('^\\d{1,}[st|nd|rd|th]$', token):\n",
    "                # Collect token synsets from Wordnet\n",
    "                syn_set = wn.synsets(token)\n",
    "                if syn_set:\n",
    "                    synonyms = []\n",
    "                    for syn in syn_set:\n",
    "                        #syn_name = syn.name().split('.')[0]\n",
    "                        #print(syn_name, syn_lemmas)\n",
    "                        # Collect synonyms from all synsets\n",
    "                        synonyms += [l.name() for l in syn.lemmas()]\n",
    "                    synonyms = list(set(synonyms))\n",
    "                    # Exclude multi word synonyms (e.g. 'survival_of_the_fittest'), \n",
    "                    markers =  ['_', '-']\n",
    "                    #synonyms = [s for s in synonyms if '_' not in s]\n",
    "                    multi_words = [s for s in synonyms if any(m in s for m in markers)] \n",
    "                    synonyms = [s for s in synonyms if s not in multi_words]\n",
    "                    # Exclude proper nouns, such as names (e.g. Clarence_Day) and capitalized forms\n",
    "                    synonyms = [s for s in synonyms if s == s.lower()]\n",
    "                    # Exclude numeric synonyms\n",
    "                    synonyms = [s for s in synonyms if type(isint(s))!=int]\n",
    "                    # Exclude single character synonyms\n",
    "                    synonyms = [s for s in synonyms if len(s)>1]\n",
    "                    # Exclude token from synonyms \n",
    "                    synonyms = [s for s in synonyms if s != token]\n",
    "                    if synonyms:\n",
    "                        for synonym in synonyms:\n",
    "                            synonyms_tokens[synonym]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "802bc114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "lemmas = list(set(pos_df['lemma']))\n",
    "\n",
    "for lemma in lemmas:\n",
    "    # Exclude numeric lemmas\n",
    "    if type(isint(lemma))!=int:\n",
    "        # Exclude single character lemmas\n",
    "        if len(lemma)>1:\n",
    "            # Exclude 2nd 3rd 4th\n",
    "            if not re.findall('^\\d{1,}[st|nd|rd|th]$', lemma):\n",
    "                # Collect lemma synsets from Wordnet\n",
    "                syn_set = wn.synsets(lemma)\n",
    "                if syn_set:\n",
    "                    synonyms = []\n",
    "                    for syn in syn_set:\n",
    "                        #syn_name = syn.name().split('.')[0]\n",
    "                        #print(syn_name, syn_lemmas)\n",
    "                        # Collect synonyms from all synsets\n",
    "                        synonyms += [l.name() for l in syn.lemmas()]\n",
    "                    synonyms = list(set(synonyms))\n",
    "                    # Exclude multi word synonyms (e.g. 'survival_of_the_fittest'), \n",
    "                    markers =  ['_', '-']\n",
    "                    #synonyms = [s for s in synonyms if '_' not in s]\n",
    "                    multi_words = [s for s in synonyms if any(m in s for m in markers)] \n",
    "                    synonyms = [s for s in synonyms if s not in multi_words]\n",
    "                    # Exclude proper nouns, such as names (e.g. Clarence_Day) and capitalized forms\n",
    "                    synonyms = [s for s in synonyms if s == s.lower()]\n",
    "                    # Exclude numeric synonyms\n",
    "                    synonyms = [s for s in synonyms if type(isint(s))!=int]\n",
    "                    # Exclude single character synonyms\n",
    "                    synonyms = [s for s in synonyms if len(s)>1]\n",
    "                    # Exclude lemma from synonyms \n",
    "                    synonyms = [s for s in synonyms if s != lemma]\n",
    "                    if synonyms:\n",
    "                        for synonym in synonyms:\n",
    "                            results.append([lemma, synonym])\n",
    "lemmas_synonyms = pd.DataFrame(results, columns = ['lemma', 'synonym'])\n",
    "lemmas_synonyms.to_excel(os.path.join(results_dir, 'lemmas_synonyms.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f5942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
