{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python\n",
    "#!pip install ipywidgets==7.6.0\n",
    "#!pip install nltk\n",
    "#!pip install boto3\n",
    "#!pip install pika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from IPython.display import display, HTML, clear_output, display_html\n",
    "from itertools import chain,cycle\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, Layout, HBox, VBox\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', 100)\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import ast\n",
    "import mysql.connector as mysql\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import boto3\n",
    "import threading\n",
    "import pika\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "menu_files = ['files','cluster_names', 'names']\n",
    "for menu_file in menu_files: \n",
    "    with open('./tmp/{mf}.txt'.format(mf=menu_file), 'w') as f: f.write('None')\n",
    "    \n",
    "# Widget styles\n",
    "style = {'description_width': 'initial'}\n",
    "features_layout = {'width': 'max-content','height':'200px'}\n",
    "\n",
    "# File selection menu\n",
    "ds_bucket = 'foresight-ds-docs'\n",
    "aws_access_key_id='AKIAQIALQA3XKOG2MNFS'\n",
    "aws_secret_access_key='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'\n",
    "s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "ds_bucket_obj = s3.Bucket(ds_bucket)\n",
    "matrices_dir = 'matrices'\n",
    "\n",
    "\n",
    "# Analyse button \n",
    "run_button = widgets.Button(description = \"Cluster\",style=style)\n",
    "run_button.style.button_color = 'lightgreen'\n",
    "# Metric menus\n",
    "metrics_layout = {'display':'flex','width': '130px','height':'30px', 'justify_content':'flex-end'}\n",
    "options = list(np.arange(1,11))\n",
    "options = [str(o) for o in options]\n",
    "metrics_optimize = {'min_max_tpc': ('min', 1), 'wcss': ('min', 1), 'bcss': ('max', 1), 'ch_index': ('max', 1),\\\n",
    "'db_index':('min', 1), 'silhouette':('max', 1), 'words_pairs': ('max', 1)}\n",
    "metrics = list(metrics_optimize.keys())\n",
    "metrics_menus = {}\n",
    "for metric in metrics:\n",
    "    menu=widgets.Dropdown(options=options,value='1',description=metric, layout=metrics_layout)\n",
    "    metrics_menus[metric]=menu\n",
    "# Granularity slider\n",
    "granularity = widgets.IntSlider(value=100, min=2, max=1000, step=1, description='Number of Clusters',\\\n",
    "                                     orientation='horizontal',readout=True, readout_format='d',\\\n",
    "                                     style = {'description_width': 'initial'}, layout=Layout(width='400px'))\n",
    "apply_granularity = widgets.ToggleButton(value=False, description='Select granularity level?',\n",
    "    disabled=False, button_style='info', tooltip='Description',\n",
    "    icon='check', layout=Layout(width='200px'))\n",
    "# Minimal cluster size\n",
    "min_cluster_menu=widgets.Dropdown(options=['0']+ options,value='0',\\\n",
    "                       description='Minimum number of tasks in cluster',\\\n",
    "                                   style = {'description_width': 'initial'},\\\n",
    "                                   layout=Layout(width='300px'))\n",
    "\n",
    "# Service    \n",
    "metrics_optimize = {'min_max_tpc': ('min', 1), 'wcss': ('min', 1), 'bcss': ('max', 1), 'ch_index': ('max', 1),\\\n",
    "'db_index':('min', 1), 'silhouette':('max', 1), 'words_pairs': ('max', 1)}\n",
    "db_name = 'CAdb'\n",
    "location_db_params = {'Local': {'host': 'localhost', 'user':'rony', 'password':'exp8546$fs', 'database': db_name},\\\n",
    "                      'Remote': {'host': '172.31.36.11', 'user':'researchUIuser', 'password':'query1234$fs', 'database': db_name}}\n",
    "location_url = {'Local': 'http://127.0.0.01:6002/cluster_analysis/api/v0.1/clustering',\\\n",
    "                'Remote': 'http://172.31.36.11/cluster_analysis/api/v0.1/clustering'}\n",
    "\n",
    "## Distance matrices\n",
    "distance_matrices = []\n",
    "matrices_paths = []\n",
    "# Distance matrices paths\n",
    "for object_summary in ds_bucket_obj.objects.filter(Prefix=matrices_dir):\n",
    "    file_key = object_summary.key\n",
    "    if file_key.split('/')[1]:\n",
    "        matrices_paths.append(file_key)\n",
    "# Load distance matrices\n",
    "for matrix_path in matrices_paths:\n",
    "    matrix_file = matrix_path.split('/')[1]\n",
    "    s3.Bucket(ds_bucket).download_file(matrix_path, matrix_file)\n",
    "    distance_matrices.append(pd.read_pickle(matrix_file))\n",
    "    os.remove(matrix_file)\n",
    "\n",
    "## Cluster naming\n",
    "from nltk.corpus import stopwords\n",
    "punctuation_marks=\"=|\\+|_|\\.|:|\\/|\\*|\\'|,|\\?\"\n",
    "def isfloat(value):\n",
    "    '''\n",
    "    Check if the input value type is float\n",
    "    '''\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def isint(value):\n",
    "    '''\n",
    "    Check if the input value type is integer\n",
    "    '''\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def split_tokens (tokens, splitter):\n",
    "    tokens_splitter= [t for t in tokens if splitter in t]\n",
    "    tokens = [t for t in tokens if splitter not in t]\n",
    "    for t in tokens_splitter: tokens += t.split(splitter)\n",
    "    return tokens\n",
    "\n",
    "def normalize_entities(name, punctuation_symbols=punctuation_marks):\n",
    "    '''\n",
    "    Identify names in tokens by the presence of symbols\n",
    "    '''\n",
    "    #print('normalize f')\n",
    "    #print('name:', name)\n",
    "    name = name.replace('&amp','')\n",
    "    tokens = name.split(' ')\n",
    "    for token in tokens:\n",
    "        if re.findall('\\d', token):\n",
    "            if re.findall('[A-Za-z]', token):\n",
    "                name = name.replace(token, '<name>')\n",
    "            else:\n",
    "                name = name.replace(token, '<number')\n",
    "        elif re.findall(punctuation_symbols, token):\n",
    "            name = name.replace(token, '<name>')\n",
    "    name = name.replace('<name> <name>', '<name>').replace('<number> <number>', '<number>')\n",
    "\n",
    "    return name\n",
    "\n",
    "def tokenize(data, unique=False, is_list=False, exclude_stopwords=False, exclude_chars=True,\\\n",
    "              split_backslah=True, split_hyphen=True, split_plus=True,\\\n",
    "              clean_punctuation=False, exclude_numbers=False, exclude_digit_tokens=False, \\\n",
    "              punctuation_symbols=punctuation_marks, stopwords=set(stopwords.words('english')),\\\n",
    "             normalized_entities=True):\n",
    "\n",
    "    if is_list:\n",
    "        data = [t for t in data if type(t)==str]\n",
    "        data = ' '.join(data)\n",
    "        data = re.sub('\\s{2,}', ' ', data)\n",
    "\n",
    "    # if exclude_parenthesis_terms:\n",
    "    #     pattern= '\\(.+?\\)|\\w*\\d{1,}\\.*\\d{1,}\\w*|\\w+'\n",
    "    #     data= re.sub(data, '', pattern)\n",
    "\n",
    "    if normalized_entities:\n",
    "        data = normalize_entities(data)\n",
    "        pattern = '\\<.+?\\>|\\w*\\d{1,}\\.*\\d{1,}\\w*|\\w+'\n",
    "        tokenizer = nltk.RegexpTokenizer(pattern)\n",
    "        tokens = tokenizer.tokenize(data)\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(data)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    if split_backslah: tokens = split_tokens (tokens, '/')\n",
    "    if split_hyphen: tokens = split_tokens(tokens, '-')\n",
    "    if split_plus: tokens = split_tokens(tokens, '+')\n",
    "\n",
    "    if exclude_stopwords: tokens = [t for t in tokens if t not in stopwords]\n",
    "    if clean_punctuation: tokens = [re.sub(punctuation_symbols, '', t) for t in tokens]\n",
    "    if exclude_chars:tokens = [t for t in tokens if len(t) > 1]\n",
    "    if exclude_numbers:\n",
    "        tokens = [t for t in tokens if (not(isint(t)))]\n",
    "        tokens = [t for t in tokens if (not(isfloat(t)))]\n",
    "    if exclude_digit_tokens: tokens = [t for t in tokens if not re.findall('\\d', t)]\n",
    "    # Unique tokens preserving the tokens order in the input text\n",
    "    if unique: tokens = sorted(set(tokens), key=tokens.index)\n",
    "    return tokens\n",
    "\n",
    "def tokens_count(tokens):\n",
    "    counts = dict()\n",
    "    for token in tokens:\n",
    "        if token in counts:\n",
    "            counts[token] += 1\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def tokens_count(tokens):\n",
    "    counts = dict()\n",
    "    for token in tokens:\n",
    "        if token in counts:\n",
    "            counts[token] += 1\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "    return counts\n",
    "\n",
    "def get_tokens_locations(parts):\n",
    "    tokens_locations = defaultdict(list)\n",
    "    for part in parts:\n",
    "        tokens = tokenize(part, unique=True, exclude_stopwords=False, \\\n",
    "                          exclude_numbers=True, exclude_digit_tokens=True)\n",
    "        tokens_indices = [tokens.index(t) for t in tokens]\n",
    "        for token in tokens:\n",
    "            tokens_locations[token].append(tokens_indices[tokens.index(token)])\n",
    "    tokens_typical_locations = {}\n",
    "    for token, locations in tokens_locations.items():\n",
    "        token_typical_location = max(set(locations), key=locations.count)\n",
    "        tokens_typical_locations[token] = token_typical_location\n",
    "\n",
    "    return tokens_typical_locations\n",
    "\n",
    "def text_to_key(cluster_names, cutoff=0.4):\n",
    "    cluster_key = ''\n",
    "    names_tokens = {}\n",
    "    for name in cluster_names:\n",
    "        tokens = tokenize(name, unique=True, exclude_stopwords=False, \\\n",
    "                           exclude_numbers=True, exclude_digit_tokens=True)\n",
    "        names_tokens[name] = tokens\n",
    "    #print('names_tokens:', names_tokens)\n",
    "    cluster_names_pairs = tuple(combinations(cluster_names, 2))\n",
    "    pairs_matches = []\n",
    "    for name_pair in cluster_names_pairs:\n",
    "        name1, name2 = name_pair\n",
    "        tokens1, tokens2 = names_tokens[name1], names_tokens[name2]\n",
    "        tokens1 = [t.lower() for t in tokens1]\n",
    "        tokens2 = [t.lower() for t in tokens2]\n",
    "        if name1 == name2:\n",
    "            pair_matches = tokens1\n",
    "        else:\n",
    "            len1, len2 = len(tokens1), len(tokens2)\n",
    "            if len1 <= len2:\n",
    "                short_name_tokens, long_name_tokens = tokens1, tokens2\n",
    "            else: short_name_tokens, long_name_tokens = tokens2, tokens1\n",
    "            pair_matches = []\n",
    "            for short_name_token in short_name_tokens:\n",
    "                short_name_token = [short_name_token]\n",
    "                names_token_pairs = list(itertools.product(short_name_token, long_name_tokens))\n",
    "                token_pairs_scores = {}\n",
    "                for tokens_pair in names_token_pairs:\n",
    "                    # Use distance matrices to score token pairs\n",
    "                    token1, token2 = tokens_pair\n",
    "                    token_pairs_score = 0\n",
    "                    for index, matrix in enumerate(distance_matrices):\n",
    "                        if all(x in matrix.columns for x in tokens_pair):\n",
    "                            matrix_score = matrix.at[token1, token2]\n",
    "                        else: matrix_score = 0\n",
    "                        token_pairs_score += matrix_score\n",
    "                    token_pairs_score = round(token_pairs_score, 2)\n",
    "                    token_pairs_scores[tokens_pair] = token_pairs_score\n",
    "\n",
    "                # Identify the best match in the long name to the short name token\n",
    "                max_score = max(list(token_pairs_scores.values()))\n",
    "                if max_score >= cutoff:\n",
    "                    for tokens_pair, pair_score in token_pairs_scores.items():\n",
    "                        if pair_score == max_score: matched_token = tokens_pair[1]\n",
    "                    #print('matched token with best score:', matched_token)\n",
    "                    pair_matches.append(matched_token)\n",
    "\n",
    "        pairs_matches.append(tuple(pair_matches))\n",
    "    matches_tokens = []\n",
    "    for pair_matches in pairs_matches: matches_tokens += list(pair_matches)\n",
    "    matches_tokens_counts = tokens_count(matches_tokens)\n",
    "\n",
    "    # Score each match by the frequency of its tokens\n",
    "    match_scores = {}\n",
    "    for pair_matches in pairs_matches:\n",
    "        match_score = 0\n",
    "        for token in pair_matches:\n",
    "            match_score += matches_tokens_counts[token]\n",
    "        match_scores[pair_matches] = match_score\n",
    "\n",
    "    # Score each match by it's length in relation to the cluster_key lengths\n",
    "    names = []\n",
    "    for name_pair in cluster_names_pairs: names += name_pair\n",
    "    names_lengths_median = np.median(np.array([len(name) for name in names]))\n",
    "    for pair_matches in pairs_matches:\n",
    "        if names_lengths_median>0:\n",
    "            near_median_factor = len(pair_matches)/names_lengths_median\n",
    "            match_scores[pair_matches] = near_median_factor * match_scores[pair_matches]\n",
    "        else: match_scores[pair_matches] = 0\n",
    "    # Identify the best scoring match\n",
    "    max_score = max(list(match_scores.values()))\n",
    "    for pair_matches, match_score in match_scores.items():\n",
    "        if match_score == max_score:\n",
    "            cluster_key = pair_matches\n",
    "\n",
    "    cluster_key = ' '.join(list(set(cluster_key)))\n",
    "    return cluster_key\n",
    "\n",
    "def parts_to_texts(cluster_names):\n",
    "    '''\n",
    "    Split a group of using a splitter symbol (e.g. hyphen) to produce lists of the phrase parts\n",
    "    Splitter: ' - '\n",
    "    '''\n",
    "    # Store names parts by their location relative to a hyphen break in each name\n",
    "    names_parts = defaultdict(list)\n",
    "\n",
    "    for name in cluster_names:\n",
    "        delimiters = ' - |/|\\(|\\)|\\[|\\]' # To keep parenthesis use ' - |/|,(\\(.+?\\))'\n",
    "        name_split = [i.rstrip().lstrip() for i in re.split(delimiters, name) if i]\n",
    "\n",
    "        # Number of parts produced by a hyphen break\n",
    "        num_parts = len(name_split)\n",
    "        parts_indices = np.arange(num_parts)\n",
    "        for index in parts_indices:\n",
    "            names_parts[index].append(name_split[index])\n",
    "    names_parts = dict(names_parts)\n",
    "    key_parts = ['']\n",
    "    for index, names_part in names_parts.items():\n",
    "        if len(names_part) > 1:\n",
    "            # Get key by the name part\n",
    "            parts_key = text_to_key(names_part, cutoff=0.8)\n",
    "            if parts_key:\n",
    "                part_key_tokens = tokenize(parts_key, unique=True, exclude_stopwords=False, \\\n",
    "                                           exclude_numbers=True, exclude_digit_tokens=True)\n",
    "                # Re-order the key words by their typical order in the name parts\n",
    "                tokens_typical_locations = get_tokens_locations(names_part)\n",
    "                key_tokens_locations = {k: v for k, v in tokens_typical_locations.items() if k in part_key_tokens}\n",
    "                sorted_key_tokens_locations = {k: v for k, v in sorted(key_tokens_locations.items(), key=lambda item: item[1])}\n",
    "                parts_key = ' '.join(list(sorted_key_tokens_locations.keys()))\n",
    "                parts_key = string.capwords(parts_key)\n",
    "                key_parts.append(parts_key)\n",
    "    key_parts = [i for i in key_parts if i]\n",
    "    if not key_parts:\n",
    "        if normalize_entities(cluster_names[0]):\n",
    "            key_parts = [normalize_entities(cluster_names[0])]\n",
    "    entity_labels = ['<number><name>', '<name><number>', '<name>', '<number>']\n",
    "    key_parts1 = []\n",
    "    for key_part in key_parts:\n",
    "        key_part = key_part.replace('> <', '><')\n",
    "        # Clear entity or number tags if they open a name part\n",
    "        for label in entity_labels:\n",
    "            label_pattern = '^\\s*{p}*\\s*{l}+'.format(l=label, p=punctuation_marks)\n",
    "            if re.findall(label_pattern, key_part):\n",
    "                key_part = re.sub(label_pattern, '', key_part)\n",
    "                key_part = key_part.lstrip().rstrip()\n",
    "        key_parts1.append(key_part)\n",
    "    key_parts1 = [p for p in key_parts1 if p]\n",
    "    key = ' - '.join(key_parts1)\n",
    "    key = key.replace('&amp', '')\n",
    "    key = re.sub('/|,|;', '-', key)\n",
    "\n",
    "    key = re.sub('^[\\s|{p}|-]*'.format(p=punctuation_marks), '', key)\n",
    "    key = key.lstrip('-')\n",
    "    return key\n",
    "\n",
    "###        \n",
    "exchange = 'kc.ca.exchange'\n",
    "def get_results(experiment_id, conn):\n",
    "    def print_message(channel, method, properties, body):\n",
    "        message = json.loads(body)\n",
    "        run_cols = ['run_start', 'run_end', 'duration', 'tasks_count']\n",
    "        result_df = pd.read_sql_query(\"SELECT * FROM results \\\n",
    "        WHERE experiment_id={eid}\".format(eid=experiment_id), conn).drop(run_cols, axis=1)\n",
    "        best_run_id = result_df['run_id'].values[0]\n",
    "        print('Run id for the best run=', best_run_id)\n",
    "        print('The clusters for the best run are ready for drill down analysis')\n",
    "        \n",
    "        # Show runs results\n",
    "        display(HTML('<h1 style=\"color:magenta\">Run Scores </h1>'))\n",
    "        print('Run for experiment {id}'.format(id=experiment_id))\n",
    "        runs_df = pd.read_sql_query(\"SELECT * FROM runs \\\n",
    "        WHERE experiment_id={eid}\".format(eid=experiment_id), conn).drop(run_cols, axis=1)\n",
    "        display(runs_df)\n",
    "        channel.queue_delete(queue=queue)\n",
    "\n",
    "    queue ='experiment_{id}'.format(id=experiment_id)\n",
    "    # Consumer\n",
    "    credentials = pika.PlainCredentials('rnd', 'Rnd@2143')\n",
    "    parameters = pika.ConnectionParameters('172.31.34.107', 5672, '/', credentials)\n",
    "    connection = pika.BlockingConnection(parameters)\n",
    "    channel = connection.channel()\n",
    "    channel.queue_declare(queue=queue, auto_delete=False)\n",
    "    channel.exchange_declare(exchange=exchange, durable=True, exchange_type='direct')\n",
    "    channel.basic_consume(queue, print_message, auto_ack=True)\n",
    "    t1 = threading.Thread(target=channel.start_consuming)\n",
    "    t1.start()\n",
    "    t1.join(0)\n",
    "\n",
    "data_path = './data/experiments'\n",
    "def run_service(b):\n",
    "    service_location = service_button.value\n",
    "    file_source = file_source_button.value\n",
    "    print('service location:', service_location)\n",
    "    print('file source location:', file_source)\n",
    "    conn_params = location_db_params[service_location]\n",
    "    conn = mysql.connect(**conn_params)\n",
    "    c=conn.cursor()\n",
    "    c.execute(\"SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED\")\n",
    "    url = location_url[service_location]\n",
    "    file_checkpoints = True\n",
    "    ## Submitted data files\n",
    "    files = file_dd.value\n",
    "    num_files = len(files)\n",
    "    print('{n} files submitted:'.format(n=num_files), (',').join(list(files)).rstrip(','))\n",
    "    file_types = list(set([t.split('.')[1] for t in files]))\n",
    "    print('file_types:', file_types)\n",
    "    #Checkpoint: Files submitted\n",
    "    if files[0][0] == ' ':\n",
    "        print('No file selected')\n",
    "        file_checkpoints = False\n",
    "    #Checkpoint: Zip files\n",
    "    elif 'zip' in file_types:\n",
    "        print('zip in file types')\n",
    "        #Checkpoint: One among few files zipped \n",
    "        if num_files==1:\n",
    "            file = files[0]\n",
    "            tmp_file_path = os.path.join('./tmp', file) \n",
    "            file_path = os.path.join(data_path, file)\n",
    "            if file_source == 'Local':\n",
    "                shutil.copy2(file_path, tmp_file_path)\n",
    "                files = {'file': open(tmp_file_path, 'rb')} \n",
    "            else:\n",
    "                s3.Bucket(ds_bucket).download_file(file, tmp_file_path)\n",
    "                files = {'file': open(file_path, 'rb')}                \n",
    "        elif num_files>1:\n",
    "            print('The submitted files include a zip file')\n",
    "            file_checkpoints = False\n",
    "    \n",
    "    #Zip the data files \n",
    "    else:\n",
    "        file_paths = []\n",
    "        for file in files:\n",
    "            tmp_file_path = os.path.join('./tmp', file)\n",
    "            file_paths.append(tmp_file_path)\n",
    "            if file_source == 'Local':\n",
    "                file_path = os.path.join(data_path, file)\n",
    "                shutil.copy2(file_path, tmp_file_path) \n",
    "            else:\n",
    "                s3.Bucket(ds_bucket).download_file(file, tmp_file_path)\n",
    "        with ZipFile('zipped_files.zip','w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file_path in file_paths:\n",
    "                zip.write(file_path)\n",
    "        files = {'file': open('zipped_files.zip', 'rb')}\n",
    "        os.remove('zipped_files.zip')\n",
    "    \n",
    "    if file_checkpoints:\n",
    "        ## Experiment configuration\n",
    "        config = {}\n",
    "        config['client'] = 'ui'\n",
    "        # Experiment id\n",
    "        experiment_ids = pd.read_sql_query(\"SELECT experiment_id from experiments\", conn).astype(int)\n",
    "        if len(experiment_ids) == 0: experiment_id = 1\n",
    "        else: experiment_id = int(max(experiment_ids.values)[0]) + 1\n",
    "        \n",
    "        config['service_location'] = service_location\n",
    "        config['experiment_id'] = experiment_id\n",
    "        print('experiment_id:', experiment_id)\n",
    "        \n",
    "        min_cluster_size = min_cluster_menu.value[0]\n",
    "        print('min_cluster_size:', min_cluster_size)\n",
    "        config['min_cluster_size'] = min_cluster_size\n",
    "\n",
    "        # Metrics weights\n",
    "        for metric, menu in metrics_menus.items():\n",
    "            config[metric] = menu.value[0]\n",
    "        if apply_granularity.value:\n",
    "            config['num_clusters'] = granularity.value\n",
    "        \n",
    "        # Post experiment data and configuration\n",
    "        response = requests.post(url, files=files, data=config)\n",
    "        print(response.text)\n",
    "        if response.text == 'Running clustering pipeline': \n",
    "            get_results(experiment_id, conn)\n",
    "                \n",
    "def left_align(df):\n",
    "    left_aligned_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "    left_aligned_df = left_aligned_df.set_table_styles(\n",
    "        [dict(selector='th', props=[('text-align', 'left')])]\n",
    "    )\n",
    "    return left_aligned_df\n",
    "\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:right\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2>{title}</h2>'\n",
    "        df=left_align(df)\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color:magenta\">Files and Service Location</h1>                 <ul>                  <li style=\"color:blue\">The service runs remotely by default</li>                  <li style=\"color:blue\">Remote files are files located on AWS</li>                  <li style=\"color:blue\">To use local files store them in ./data/experiment</li>                </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7312c6c1b4d418595f3bb56e35d9a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='File Source:', layout=Layout(width='max-content'), options=('Local', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_files_list(b): \n",
    "    data_files = [' ']\n",
    "    if file_source_button.value == 'Local':\n",
    "        data_files += os.listdir(data_path)\n",
    "    else:\n",
    "        for key in s3_client.list_objects(Bucket=ds_bucket)['Contents']:\n",
    "            data_files.append(key['Key'])\n",
    "    data_files = '\\n'.join(data_files)\n",
    "    with open(os.path.join('./tmp', 'files.txt'), 'w') as f: f.write(data_files)\n",
    "\n",
    "display(HTML('<h1 style=\"color:magenta\">Files and Service Location</h1>\\\n",
    "                 <ul>\\\n",
    "                  <li style=\"color:blue\">The service runs remotely by default</li>\\\n",
    "                  <li style=\"color:blue\">Remote files are files located on AWS</li>\\\n",
    "                  <li style=\"color:blue\">To use local files store them in ./data/experiment</li>\\\n",
    "                </ul>'))\n",
    "file_source_button = widgets.RadioButtons(\n",
    "    options=['Local', 'Remote'], value='Local',layout={'width': 'max-content'}, description='File Source:',\n",
    "    disabled=False)\n",
    "service_button = widgets.RadioButtons(\n",
    "    options=['Local', 'Remote'], value='Remote',layout={'width': 'max-content'}, description='Service:',\n",
    "    disabled=False)\n",
    "location_button = widgets.Button(description = \"Set Locations\",style=style, layout={'width': 'max-content'})\n",
    "location_button.style.button_color = 'lightgreen'\n",
    "location_button.on_click(build_files_list)\n",
    "HBox(children=[file_source_button, service_button, location_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color:magenta\">Cluster Activities</h1>              <p style=\"color:blue\">Use to following menus to submit a file for analysis:</p>                 <ul>                  <li style=\"color:magenta\">File to analyze</li>                  <li style=\"color:magenta\">Select granularity level</li>                  <li style=\"color:magenta\">Set weights for validation metrics</li>                </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98535511e11f414e91959011b1039a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(SelectMultiple(description='File:', index=(0, 0), layout=Layout(height='200px', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service location: Remote\n",
      "file source location: Local\n",
      "1 files submitted: CCGTD1_IPS_sample.zip\n",
      "file_types: ['zip']\n",
      "zip in file types\n",
      "experiment_id: 21\n",
      "min_cluster_size: 0\n",
      "Running clustering pipeline\n"
     ]
    }
   ],
   "source": [
    "# Dashboard\n",
    "run_button.on_click(run_service)\n",
    "display(HTML('<h1 style=\"color:magenta\">Cluster Activities</h1>\\\n",
    "              <p style=\"color:blue\">Use to following menus to submit a file for analysis:</p>\\\n",
    "                 <ul>\\\n",
    "                  <li style=\"color:magenta\">File to analyze</li>\\\n",
    "                  <li style=\"color:magenta\">Select granularity level</li>\\\n",
    "                  <li style=\"color:magenta\">Set weights for validation metrics</li>\\\n",
    "                </ul>'))\n",
    "data_files = [i for i in ([' '] + open('./tmp/files.txt').read().split('\\n')) if i]\n",
    "default = (data_files[0], ' ')\n",
    "file_dd=widgets.SelectMultiple(options=data_files,value=default,\n",
    "    description='File:',style=style,layout=features_layout)\n",
    "file_box = VBox(children=[file_dd, run_button])\n",
    "metrics_box = VBox(children=list(metrics_menus.values()))\n",
    "config_box = VBox(children=[apply_granularity, granularity, min_cluster_menu])\n",
    "HBox(children=[file_box, config_box, metrics_box])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100a04fdf4c8456d9b177a41f723cd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Sort By (Desc):', layout=Layout(width='max-content'), options=('Alphabeti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def result_from_table(experiment_id, result_key='clusters'):\n",
    "    result_df = pd.read_sql_query(\"SELECT * FROM results \\\n",
    "    WHERE experiment_id={eid}\".format(eid=experiment_id), conn)\n",
    "    result = result_df['result'].values[0]\n",
    "    result = ast.literal_eval(result)\n",
    "    if result_key == 'clusters':    \n",
    "        result_key = [c for c in result.keys() if 'duration' not in c][0]\n",
    "    return result[result_key]\n",
    "\n",
    "\n",
    "def get_clusters(b):\n",
    "    experiment_id = experiments_dd.value\n",
    "    result_df = pd.read_sql_query(\"SELECT * FROM results \\\n",
    "    WHERE experiment_id={eid}\".format(eid=experiment_id), conn)\n",
    "    best_run_id = result_df['run_id'].values[0]\n",
    "    print('best run id=', best_run_id)\n",
    "    result = result_df['result'].values[0]\n",
    "    file_key = result_df['file_name'].values[0]\n",
    "    result = ast.literal_eval(result)\n",
    "    result = result[file_key]\n",
    "    #print('result:', result)\n",
    "    clusters_keys = list(result.keys())\n",
    "    sort_by = clusters_sort_dd.value\n",
    "    if sort_by == 'Alphabetic Order':\n",
    "        clusters_keys.sort()\n",
    "    else:\n",
    "        # Calculate %Overrun for sorting\n",
    "        planned_duration_dict = result_from_table(experiment_id, 'planned_duration_vals') \n",
    "        actual_duration_dict = result_from_table(experiment_id, 'actual_duration_vals') \n",
    "        clusters = result_from_table(experiment_id)\n",
    "        clusters_overruns = {}\n",
    "        for cluster_key in clusters_keys:\n",
    "            ids_names = clusters[cluster_key]\n",
    "            #print('ids_names:', ids_names)\n",
    "            cluster_ids = [i[0] for i in ids_names]\n",
    "            #print(cluster_ids)\n",
    "            cluster_planned_duration_dict = {k:v for k,v in planned_duration_dict.items() if k in cluster_ids}\n",
    "            cluster_actual_duration_dict = {k:v for k,v in actual_duration_dict.items() if k in cluster_ids}\n",
    "            #print('actual_duration_dict:', actual_duration_dict)\n",
    "            planned_in_actual_dict = {k:v for k,v in cluster_planned_duration_dict.items()\\\n",
    "                                      if k in cluster_actual_duration_dict.keys()}\n",
    "            tasks_overrun_perc = []\n",
    "            for id, task_planned_duration in planned_in_actual_dict.items():\n",
    "                task_actual_duration = actual_duration_dict[id]\n",
    "    #             print('id, task_actual_duration, task_planned_duration:', id,\\\n",
    "    #                   task_actual_duration, task_planned_duration)\n",
    "                if task_planned_duration != 0:\n",
    "                    task_overrun_perc = 100*((task_actual_duration/task_planned_duration)-1)\n",
    "    #                 if task_overrun_perc>1000:\n",
    "    #                     print('task:', id)\n",
    "    #                     print('task_overrun_perc:', task_overrun_perc)\n",
    "    #                     print('task_actual_duration, task_planned_duration:', task_actual_duration, task_planned_duration)\n",
    "                    tasks_overrun_perc.append(task_overrun_perc)\n",
    "            if tasks_overrun_perc:\n",
    "                mean_perc_overrun = round(int(np.mean(tasks_overrun_perc)), 2)\n",
    "            else: mean_perc_overrun = 0 \n",
    "            clusters_overruns[cluster_key] = mean_perc_overrun    \n",
    "        clusters_overruns = {k: v for k, v in sorted(clusters_overruns.items(),\\\n",
    "                                                     key=lambda item: item[1], reverse=True)}\n",
    "        #print('clusters_overruns:', clusters_overruns)\n",
    "        clusters_keys = list(clusters_overruns.keys())\n",
    "    clusters_keys = '\\n'.join(clusters_keys)\n",
    "    with open('./tmp/cluster_names.txt', 'w') as f: f.write(clusters_keys)\n",
    "\n",
    "#display(HTML('<h1 style=\"color:magenta\">Select Experiment</h1>'))\n",
    "service_location = service_button.value\n",
    "conn_params = location_db_params[service_location]\n",
    "conn = mysql.connect(**conn_params)\n",
    "experiment_ids = pd.read_sql_query(\"SELECT experiment_id FROM experiments\", conn).astype(int)\n",
    "experiment_ids = list(experiment_ids['experiment_id'].unique())\n",
    "if len(experiment_ids) == 0: experiment_ids = ['None']\n",
    "experiments_dd=widgets.Dropdown(options=experiment_ids, value=experiment_ids[0],\n",
    "    description='Experiment:',style=style, layout={'width': 'max-content'})\n",
    "sort_options = ['Alphabetic Order', 'Percent Overrun Mean Percentage']\n",
    "clusters_sort_dd=widgets.Dropdown(options=sort_options, value=sort_options[0],\n",
    "    description='Sort By (Desc):',style=style, layout={'width': 'max-content'})\n",
    "clusters_button = widgets.Button(description = \"Clusters \",style=style, layout={'width': 'max-content'})\n",
    "clusters_button.style.button_color = 'lightblue'\n",
    "clusters_button.on_click(get_clusters)\n",
    "HBox(children=[clusters_sort_dd, experiments_dd, clusters_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color:magenta\">Select Cluster</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0164d7d901494f981ab44079187e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(layout=Layout(width='max-content'), options=('None',), style=DescriptionStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_tasks(b):\n",
    "    cluster_key = clusters_keys_dd.value\n",
    "    print('Select cluster:', cluster_key)\n",
    "    experiment_id = experiments_dd.value\n",
    "    clusters = result_from_table(experiment_id)\n",
    "    ids_names = clusters[cluster_key]\n",
    "    ids_names = [' '.join(id_name) for id_name in ids_names]\n",
    "    ids_names = '\\n'.join(ids_names)\n",
    "    with open('./tmp/names.txt', 'w') as f: f.write(ids_names)\n",
    "        \n",
    "clusters_keys = open('./tmp/cluster_names.txt').read().split('\\n')\n",
    "clusters_keys_dd=widgets.Dropdown(options=clusters_keys, value=clusters_keys[0],\n",
    "    style=style, layout={'width': 'max-content'})\n",
    "#description='Selected Cluster:',\n",
    "tasks_button = widgets.Button(description = \"Tasks\",style=style, layout={'width': 'max-content'})\n",
    "tasks_button.style.button_color = 'orange'\n",
    "tasks_button.on_click(get_tasks)\n",
    "display(HTML('<h1 style=\"color:magenta\">Select Cluster</h1>'))\n",
    "VBox(children=[clusters_keys_dd, experiments_dd, tasks_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color:magenta\">Tasks to Exclude</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cdccb80b504b188c1151cc6f951ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Experiment:', layout=Layout(width='max-content'), options=(1, 2, 3, 4, 5,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cluster_stats_plots(b):\n",
    "    cluster_key = clusters_keys_dd.value\n",
    "    experiment_id = experiments_dd.value\n",
    "    clusters = result_from_table(experiment_id)\n",
    "    names_to_exclude = ids_names_dd.value\n",
    "    exclude_ids = [i.split(' ')[0] for i in names_to_exclude]\n",
    "    \n",
    "    # Activities duration values\n",
    "    ids_names = {i[0]:i[1] for i in clusters[cluster_key] if i[0] not in exclude_ids}\n",
    "    ids, names = list(ids_names.keys()), list(ids_names.values())\n",
    "    new_key = parts_to_texts(names)\n",
    "\n",
    "    planned_duration_dict = result_from_table(experiment_id, 'planned_duration_vals') \n",
    "    planned_duration_dict = {k:v for k,v in planned_duration_dict.items() if k in ids}\n",
    "    planned_duration_vals = list(planned_duration_dict.values())\n",
    "    \n",
    "    actual_duration_dict = result_from_table(experiment_id, 'actual_duration_vals') \n",
    "    actual_duration_dict = {k:v for k,v in actual_duration_dict.items() if k in ids}\n",
    "    actual_duration_vals = list(actual_duration_dict.values())\n",
    "    planned_in_actual_dict = {k:v for k,v in planned_duration_dict.items() if k in actual_duration_dict.keys()}\n",
    "    tasks_overrun_perc = []\n",
    "    \n",
    "    # Duration ratios and overruns\n",
    "    duration_ratios, tasks_overrun, tasks_overrun_perc = [], [], []\n",
    "    for id, task_planned_duration in planned_in_actual_dict.items():\n",
    "        task_actual_duration = actual_duration_dict[id]\n",
    "        if task_planned_duration != 0:\n",
    "            duration_ratios.append(round(task_actual_duration/task_planned_duration,2))\n",
    "            task_overrun = task_actual_duration-task_planned_duration\n",
    "            tasks_overrun.append(task_overrun)\n",
    "            tasks_overrun_perc.append(100*(task_actual_duration/task_planned_duration)-1)\n",
    "\n",
    "    ## Cluster Statistics\n",
    "    display(HTML('<h1 style=\"color:magenta\">Cluster RCF Analysis</h1>'))\n",
    "    print('New Key:', new_key)\n",
    "    print('Cluster Statistics')\n",
    "    print('Activities in Cluster:', len(ids))\n",
    "    print('Completed Activities in Cluster:', len(actual_duration_vals))\n",
    "    # Table\n",
    "    index = ['Planned Duration(Days)', 'Actual Duration(Days)', 'Overrun(Days)', 'Overrun(%)']\n",
    "    headers = ['MEAN', 'MEDIAN', 'STD']\n",
    "    def stats_row(arr): \n",
    "        if len(arr)>0:\n",
    "            return [np.mean(arr), np.median(arr), np.std(arr)]\n",
    "        else:\n",
    "            return(np.nan, np.nan, np.nan)\n",
    "    table_rows = [stats_row(planned_duration_vals), stats_row(actual_duration_vals),\\\n",
    "                 stats_row(tasks_overrun), stats_row(tasks_overrun_perc)]\n",
    "    stats_df = pd.DataFrame(table_rows, columns=headers, index=index)\n",
    "    stats_df = round(stats_df, 2)\n",
    "    #display(stats_df)                \n",
    "\n",
    "    # Plot Values: RCF for Tasks in Cluster\n",
    "    # x = percentile,  y = duration_ratios\n",
    "    duration_ratios.sort()\n",
    "    sum_ratios = sum(duration_ratios)\n",
    "    ratios_cumsum = np.cumsum(duration_ratios)\n",
    "    percentile = 100*(ratios_cumsum/sum_ratios)\n",
    "    rcf_df = pd.DataFrame(list(zip(percentile, duration_ratios)),\\\n",
    "                          columns = ['Percentile', 'Duration Ratio'])\n",
    "\n",
    "    # Plot Values: Duration Distribution\n",
    "    duration_type = len(planned_duration_vals) * ['Planned'] + len(actual_duration_vals) *['Actual']\n",
    "    duration_vals = list(planned_duration_vals)+list(actual_duration_vals)\n",
    "    duration_df = pd.DataFrame(list(zip(duration_type, duration_vals)), columns=['Duration', 'Days'])\n",
    "\n",
    "    # Cluster description\n",
    "    #print('planned_duration_vals:', planned_duration_vals)\n",
    "    #print('actual_duration_vals:', actual_duration_vals)\n",
    "    names_df = pd.DataFrame(ids, columns=['ID'])\n",
    "    names_df['Name'] = names\n",
    "    names_df['Planned Duration'] = planned_duration_vals\n",
    "    names_df['Actual Duration'] = actual_duration_vals\n",
    "\n",
    "    ## Display\n",
    "    # Tables\n",
    "    display_side_by_side(names_df,stats_df, titles=['Tasks in Cluster','Cluster Statistics'])\n",
    "\n",
    "    # Plots\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))#, sharey=True)\n",
    "    duration_dist = sns.boxplot(ax=axes[0], x=\"Duration\", y=\"Days\", data=duration_df)\n",
    "    rcf = sns.lineplot(ax=axes[1], x=\"Percentile\", y=\"Duration Ratio\", data=rcf_df)\n",
    "    axes[0].set_title('Duration Distibution (Tasks in Cluster)')\n",
    "    axes[1].set_title('RCF For Tasks in Cluster')\n",
    "\n",
    "\n",
    "ids_names = [' '] + open('./tmp/names.txt').read().split('\\n')\n",
    "layout={'width': 'max-content', 'height':'max-content'}\n",
    "ids_names_dd=widgets.SelectMultiple(options=ids_names, value=(ids_names[0], ' '),\n",
    "    style=style, layout=layout)\n",
    "button = widgets.Button(description = \"Run RCF\",style=style)\n",
    "button.style.button_color = 'yellow'\n",
    "button.on_click(cluster_stats_plots)\n",
    "display(HTML('<h1 style=\"color:magenta\">Tasks to Exclude</h1>'))\n",
    "VBox(children=[experiments_dd, ids_names_dd, button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
