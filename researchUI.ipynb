{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python\n",
    "#!pip install ipywidgets==7.6.0\n",
    "#!pip install nltk\n",
    "#!pip install boto3\n",
    "#!pip install pika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from IPython.display import display, HTML, clear_output, display_html\n",
    "from itertools import chain,cycle\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, Layout, HBox, VBox\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', 100)\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "import ast\n",
    "import mysql.connector as mysql\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import boto3\n",
    "import threading\n",
    "import pika\n",
    "import shutil\n",
    "\n",
    "if 'tmp' in os.listdir():\n",
    "    shutil.rmtree('tmp')\n",
    "os.mkdir('tmp')\n",
    "menu_files = ['files','cluster_names', 'names']\n",
    "for menu_file in menu_files: \n",
    "    with open('./tmp/{mf}.txt'.format(mf=menu_file), 'w') as f: f.write('None')\n",
    "    \n",
    "# Widget styles\n",
    "style = {'description_width': 'initial'}\n",
    "features_layout = {'width': 'max-content','height':'200px'}\n",
    "\n",
    "# File selection menu\n",
    "ds_bucket = 'foresight-ds-docs'\n",
    "aws_access_key_id='AKIAQIALQA3XKOG2MNFS'\n",
    "aws_secret_access_key='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'\n",
    "s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "# Analyse button \n",
    "run_button = widgets.Button(description = \"Cluster\",style=style)\n",
    "run_button.style.button_color = 'lightgreen'\n",
    "# Metric menus\n",
    "metrics_layout = {'display':'flex','width': '130px','height':'30px', 'justify_content':'flex-end'}\n",
    "options = list(np.arange(1,11))\n",
    "options = [str(o) for o in options]\n",
    "metrics_optimize = {'min_max_tpc': ('min', 1), 'wcss': ('min', 1), 'bcss': ('max', 1), 'ch_index': ('max', 1),\\\n",
    "'db_index':('min', 1), 'silhouette':('max', 1), 'words_pairs': ('max', 1)}\n",
    "metrics = list(metrics_optimize.keys())\n",
    "metrics_menus = {}\n",
    "for metric in metrics:\n",
    "    menu=widgets.Dropdown(options=options,value='1',description=metric, layout=metrics_layout)\n",
    "    metrics_menus[metric]=menu\n",
    "# Granularity slider\n",
    "granularity = widgets.IntSlider(value=100, min=2, max=1000, step=1, description='Number of Clusters',\\\n",
    "                                     orientation='horizontal',readout=True, readout_format='d',\\\n",
    "                                     style = {'description_width': 'initial'}, layout=Layout(width='400px'))\n",
    "apply_granularity = widgets.ToggleButton(value=False, description='Select granularity level?',\n",
    "    disabled=False, button_style='info', tooltip='Description',\n",
    "    icon='check', layout=Layout(width='200px'))\n",
    "# Minimal cluster size\n",
    "min_cluster_menu=widgets.Dropdown(options=['0']+ options,value='0',\\\n",
    "                       description='Minimum number of tasks in cluster',\\\n",
    "                                   style = {'description_width': 'initial'},\\\n",
    "                                   layout=Layout(width='300px'))\n",
    "\n",
    "# Service    \n",
    "metrics_optimize = {'min_max_tpc': ('min', 1), 'wcss': ('min', 1), 'bcss': ('max', 1), 'ch_index': ('max', 1),\\\n",
    "'db_index':('min', 1), 'silhouette':('max', 1), 'words_pairs': ('max', 1)}\n",
    "db_name = 'CAdb'\n",
    "location_db_params = {'Local': {'host': 'localhost', 'user':'rony', 'password':'exp8546$fs', 'database': db_name},\\\n",
    "                      'Remote': {'host': '172.31.36.11', 'user':'researchUIuser', 'password':'query1234$fs', 'database': db_name}}\n",
    "location_url = {'Local': 'http://127.0.0.01:6002/cluster_analysis/api/v0.1/clustering',\\\n",
    "                'Remote': 'http://172.31.36.11/cluster_analysis/api/v0.1/clustering'}\n",
    "\n",
    "matrices_dir = '/home/rony/Projects_Code/Cluster_Activities/matrices'\n",
    "distance_matrices = []\n",
    "matrices = os.listdir(matrices_dir)\n",
    "for matrix in matrices:\n",
    "    path = os.path.join(matrices_dir, matrix)\n",
    "    distance_matrices.append(pd.read_pickle(path))\n",
    "\n",
    "punctuation_marks=\"=|\\+|_|\\.|:|\\/|\\*|\\'|,|?\"\n",
    "def split_tokens (tokens, splitter):\n",
    "    tokens_splitter= [t for t in tokens if splitter in t]\n",
    "    tokens = [t for t in tokens if splitter not in t]\n",
    "    for t in tokens_splitter: tokens += t.split(splitter)\n",
    "    return tokens\n",
    "\n",
    "def isfloat(value):\n",
    "    '''\n",
    "    Check if the input value type is float\n",
    "    '''\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def isint(value):\n",
    "    '''\n",
    "    Check if the input value type is integer\n",
    "    '''\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize(data, unique=True, is_list=False,\\\n",
    "              exclude_parenthesis_terms=False, exclude_stopwords=False, exclude_chars=True,\\\n",
    "              split_backslah=True, split_hyphen=True, split_plus=True, \\\n",
    "              clean_punctuation=False, exclude_numbers=False, exclude_digit_tokens=False, \\\n",
    "              punctuation_symbols=punctuation_marks, stopwords=set(stopwords.words('english'))):\n",
    "\n",
    "    if is_list:\n",
    "        data = [t for t in data if type(t)==str]\n",
    "        data = ' '.join(data)\n",
    "        data = re.sub('\\s{2,}', ' ', data)\n",
    "\n",
    "    if exclude_parenthesis_terms:\n",
    "        pattern= '\\(.+?\\)|\\w*\\d{1,}\\.*\\d{1,}\\w*|\\w+'\n",
    "        data= re.sub(data, '', pattern)\n",
    "\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    if split_backslah: tokens = split_tokens (tokens, '/')\n",
    "    if split_hyphen: tokens = split_tokens(tokens, '-')\n",
    "    if split_plus: tokens = split_tokens(tokens, '+')\n",
    "\n",
    "    if exclude_stopwords: tokens = [t for t in tokens if t not in stopwords]\n",
    "    if clean_punctuation: tokens = [re.sub(punctuation_symbols, '', t) for t in tokens]\n",
    "    if exclude_chars:tokens = [t for t in tokens if len(t) > 1]\n",
    "    if exclude_numbers:\n",
    "        tokens = [t for t in tokens if (not(isint(t)))]\n",
    "        tokens = [t for t in tokens if (not(isfloat(t)))]\n",
    "    if exclude_digit_tokens:tokens = [t for t in tokens if not re.findall('\\d', t)]\n",
    "\n",
    "    if unique: tokens = list(set(tokens))\n",
    "    return tokens\n",
    "\n",
    "def tokens_count(tokens):\n",
    "    counts = dict()\n",
    "    for token in tokens:\n",
    "        if token in counts:\n",
    "            counts[token] += 1\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "    return counts\n",
    "\n",
    "def get_cluster_key(cluster_names, cutoff=0.8):\n",
    "    names_tokens = {}\n",
    "    for name in cluster_names:\n",
    "        tokens = tokenize(name, unique=True, exclude_stopwords=True, \\\n",
    "                           exclude_numbers=True, exclude_digit_tokens=True)\n",
    "        names_tokens[name] = tokens\n",
    "\n",
    "    cluster_names_pairs = tuple(combinations(cluster_names, 2))\n",
    "    pairs_matches = []\n",
    "    for name_pair in cluster_names_pairs:\n",
    "        name1, name2 = name_pair\n",
    "        tokens1, tokens2 = names_tokens[name1], names_tokens[name2]\n",
    "        tokens1 = [t.lower() for t in tokens1]\n",
    "        tokens2 = [t.lower() for t in tokens2]\n",
    "        if name1 == name2:\n",
    "            pair_matches = tokens1\n",
    "        else:\n",
    "            len1, len2 = len(tokens1), len(tokens2)\n",
    "            if len1 <= len2:\n",
    "                short_name_tokens, long_name_tokens = tokens1, tokens2\n",
    "            else: short_name_tokens, long_name_tokens = tokens2, tokens1\n",
    "            pair_matches = []\n",
    "            for short_name_token in short_name_tokens:\n",
    "                short_name_token = [short_name_token]\n",
    "                names_token_pairs = list(itertools.product(short_name_token, long_name_tokens))\n",
    "                token_pairs_scores = {}\n",
    "                for tokens_pair in names_token_pairs:\n",
    "                    # Use distance matrices to score token pairs\n",
    "                    token1, token2 = tokens_pair\n",
    "                    token_pairs_score = 0\n",
    "                    for index, matrix in enumerate(distance_matrices):\n",
    "                        if all(x in matrix.columns for x in tokens_pair):\n",
    "                            matrix_score = matrix.at[token1, token2]\n",
    "                        else: matrix_score = 0\n",
    "                        token_pairs_score += matrix_score\n",
    "                    token_pairs_score = round(token_pairs_score, 2)\n",
    "                    token_pairs_scores[tokens_pair] = token_pairs_score\n",
    "\n",
    "                # Identify the best match in the long name to the short name token\n",
    "                max_score = max(list(token_pairs_scores.values()))\n",
    "                if max_score >= cutoff:\n",
    "                    for tokens_pair, pair_score in token_pairs_scores.items():\n",
    "                        if pair_score == max_score: matched_token = tokens_pair[1]\n",
    "                    #print('matched token with best score:', matched_token)\n",
    "                    pair_matches.append(matched_token)\n",
    "\n",
    "        pairs_matches.append(tuple(pair_matches))\n",
    "    matches_tokens = []\n",
    "    for pair_matches in pairs_matches: matches_tokens += list(pair_matches)\n",
    "    matches_tokens_counts = tokens_count(matches_tokens)\n",
    "\n",
    "    # Score each match by the frequency of its tokens\n",
    "    match_scores = {}\n",
    "    for pair_matches in pairs_matches:\n",
    "        match_score = 0\n",
    "        for token in pair_matches:\n",
    "            match_score += matches_tokens_counts[token]\n",
    "        match_scores[pair_matches] = match_score\n",
    "\n",
    "    # Score each match by it's length in relation to the names lengths\n",
    "    names = []\n",
    "    for name_pair in cluster_names_pairs: names += name_pair\n",
    "    names_lengths_median = np.median(np.array([len(name) for name in names]))\n",
    "    for pair_matches in pairs_matches:\n",
    "        near_median_factor = len(pair_matches)/names_lengths_median\n",
    "        match_scores[pair_matches] = near_median_factor * match_scores[pair_matches]\n",
    "\n",
    "    # Identify the best scoring match\n",
    "    max_score = max(list(match_scores.values()))\n",
    "    for pair_matches, match_score in match_scores.items():\n",
    "        if match_score == max_score:\n",
    "            cluster_key = pair_matches\n",
    "    cluster_key = ' '.join(list(set(cluster_key)))\n",
    "    return cluster_key\n",
    "\n",
    "        \n",
    "exchange = 'kc.ca.exchange'\n",
    "def get_results(experiment_id, conn):\n",
    "    def print_message(channel, method, properties, body):\n",
    "        message = json.loads(body)\n",
    "        run_cols = ['run_start', 'run_end', 'duration', 'tasks_count']\n",
    "        result_df = pd.read_sql_query(\"SELECT * FROM results \\\n",
    "        WHERE experiment_id={eid}\".format(eid=experiment_id), conn).drop(run_cols, axis=1)\n",
    "        best_run_id = result_df['run_id'].values[0]\n",
    "        print('Run id for the best run=', best_run_id)\n",
    "        print('The clusters for the best run are ready for drill down analysis')\n",
    "        \n",
    "        # Show runs results\n",
    "        display(HTML('<h1 style=\"color:magenta\">Run Scores </h1>'))\n",
    "        print('Run for experiment {id}'.format(id=experiment_id))\n",
    "        runs_df = pd.read_sql_query(\"SELECT * FROM runs \\\n",
    "        WHERE experiment_id={eid}\".format(eid=experiment_id), conn).drop(run_cols, axis=1)\n",
    "        display(runs_df)\n",
    "        channel.queue_delete(queue=queue)\n",
    "\n",
    "    queue ='experiment_{id}'.format(id=experiment_id)\n",
    "    # Consumer\n",
    "    credentials = pika.PlainCredentials('rnd', 'Rnd@2143')\n",
    "    parameters = pika.ConnectionParameters('172.31.34.107', 5672, '/', credentials)\n",
    "    connection = pika.BlockingConnection(parameters)\n",
    "    channel = connection.channel()\n",
    "    channel.queue_declare(queue=queue, auto_delete=False)\n",
    "    channel.exchange_declare(exchange=exchange, durable=True, exchange_type='direct')\n",
    "    channel.basic_consume(queue, print_message, auto_ack=True)\n",
    "    t1 = threading.Thread(target=channel.start_consuming)\n",
    "    t1.start()\n",
    "    t1.join(0)\n",
    "\n",
    "data_path = './data/experiments'\n",
    "def run_service(b):\n",
    "    service_location = service_button.value\n",
    "    file_source = file_source_button.value\n",
    "    print('service location:', service_location)\n",
    "    print('file source location:', file_source)\n",
    "    conn_params = location_db_params[service_location]\n",
    "    conn = mysql.connect(**conn_params)\n",
    "    c=conn.cursor()\n",
    "    c.execute(\"SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED\")\n",
    "    url = location_url[service_location]\n",
    "    file_checkpoints = True\n",
    "    ## Submitted data files\n",
    "    files = file_dd.value\n",
    "    num_files = len(files)\n",
    "    print('{n} files submitted:'.format(n=num_files), (',').join(list(files)).rstrip(','))\n",
    "    file_types = list(set([t.split('.')[1] for t in files]))\n",
    "    print('file_types:', file_types)\n",
    "    #Checkpoint: Files submitted\n",
    "    if files[0][0] == ' ':\n",
    "        print('No file selected')\n",
    "        file_checkpoints = False\n",
    "    #Checkpoint: Zip files\n",
    "    elif 'zip' in file_types:\n",
    "        print('zip in file types')\n",
    "        #Checkpoint: One among few files zipped \n",
    "        if num_files==1:\n",
    "            file = files[0]\n",
    "            tmp_file_path = os.path.join('./tmp', file) \n",
    "            file_path = os.path.join(data_path, file)\n",
    "            if file_source == 'Local':\n",
    "                shutil.copy2(file_path, tmp_file_path)\n",
    "                files = {'file': open(tmp_file_path, 'rb')} \n",
    "            else:\n",
    "                s3.Bucket(ds_bucket).download_file(file, tmp_file_path)\n",
    "                files = {'file': open(file_path, 'rb')}                \n",
    "        elif num_files>1:\n",
    "            print('The submitted files include a zip file')\n",
    "            file_checkpoints = False\n",
    "    \n",
    "    #Zip the data files \n",
    "    else:\n",
    "        file_paths = []\n",
    "        for file in files:\n",
    "            tmp_file_path = os.path.join('./tmp', file)\n",
    "            file_paths.append(tmp_file_path)\n",
    "            if file_source == 'Local':\n",
    "                file_path = os.path.join(data_path, file)\n",
    "                shutil.copy2(file_path, tmp_file_path) \n",
    "            else:\n",
    "                s3.Bucket(ds_bucket).download_file(file, tmp_file_path)\n",
    "        with ZipFile('zipped_files.zip','w') as zip:\n",
    "            # writing each file one by one\n",
    "            for file_path in file_paths:\n",
    "                zip.write(file_path)\n",
    "        files = {'file': open('zipped_files.zip', 'rb')}\n",
    "        os.remove('zipped_files.zip')\n",
    "    \n",
    "    if file_checkpoints:\n",
    "        ## Experiment configuration\n",
    "        config = {}\n",
    "        config['client'] = 'ui'\n",
    "        # Experiment id\n",
    "        experiment_ids = pd.read_sql_query(\"SELECT experiment_id from experiments\", conn).astype(int)\n",
    "        if len(experiment_ids) == 0: experiment_id = 1\n",
    "        else: experiment_id = int(max(experiment_ids.values)[0]) + 1\n",
    "        \n",
    "        config['service_location'] = service_location\n",
    "        config['experiment_id'] = experiment_id\n",
    "        print('experiment_id:', experiment_id)\n",
    "        \n",
    "        min_cluster_size = min_cluster_menu.value[0]\n",
    "        print('min_cluster_size:', min_cluster_size)\n",
    "        config['min_cluster_size'] = min_cluster_size\n",
    "\n",
    "        # Metrics weights\n",
    "        for metric, menu in metrics_menus.items():\n",
    "            config[metric] = menu.value[0]\n",
    "        if apply_granularity.value:\n",
    "            config['num_clusters'] = granularity.value\n",
    "        \n",
    "        # Post experiment data and configuration\n",
    "        response = requests.post(url, files=files, data=config)\n",
    "        print(response.text)\n",
    "        if response.text == 'Running clustering pipeline': \n",
    "            get_results(experiment_id, conn)\n",
    "                \n",
    "def left_align(df):\n",
    "    left_aligned_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "    left_aligned_df = left_aligned_df.set_table_styles(\n",
    "        [dict(selector='th', props=[('text-align', 'left')])]\n",
    "    )\n",
    "    return left_aligned_df\n",
    "\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:right\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2>{title}</h2>'\n",
    "        df=left_align(df)\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color:magenta\">Files and Service Location</h1>                 <ul>                  <li style=\"color:blue\">The service runs remotely by default</li>                  <li style=\"color:blue\">Remote files are files located on AWS</li>                  <li style=\"color:blue\">To use local files store them in ./data/experiment</li>                </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e904036ca9f453baa12606abf4faf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(RadioButtons(description='File Source:', layout=Layout(width='max-content'), options=('Local', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_files_list(b): \n",
    "    data_files = [' ']\n",
    "    if file_source_button.value == 'Local':\n",
    "        data_files += os.listdir(data_path)\n",
    "    else:\n",
    "        for key in s3_client.list_objects(Bucket=ds_bucket)['Contents']:\n",
    "            data_files.append(key['Key'])\n",
    "    data_files = '\\n'.join(data_files)\n",
    "    with open(os.path.join('./tmp', 'files.txt'), 'w') as f: f.write(data_files)\n",
    "\n",
    "display(HTML('<h1 style=\"color:magenta\">Files and Service Location</h1>\\\n",
    "                 <ul>\\\n",
    "                  <li style=\"color:blue\">The service runs remotely by default</li>\\\n",
    "                  <li style=\"color:blue\">Remote files are files located on AWS</li>\\\n",
    "                  <li style=\"color:blue\">To use local files store them in ./data/experiment</li>\\\n",
    "                </ul>'))\n",
    "file_source_button = widgets.RadioButtons(\n",
    "    options=['Local', 'Remote'], value='Local',layout={'width': 'max-content'}, description='File Source:',\n",
    "    disabled=False)\n",
    "service_button = widgets.RadioButtons(\n",
    "    options=['Local', 'Remote'], value='Remote',layout={'width': 'max-content'}, description='Service:',\n",
    "    disabled=False)\n",
    "location_button = widgets.Button(description = \"Set Locations\",style=style, layout={'width': 'max-content'})\n",
    "location_button.style.button_color = 'lightgreen'\n",
    "location_button.on_click(build_files_list)\n",
    "HBox(children=[file_source_button, service_button, location_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color:magenta\">Cluster Activities</h1>              <p style=\"color:blue\">Use to following menus to submit a file for analysis:</p>                 <ul>                  <li style=\"color:magenta\">File to analyze</li>                  <li style=\"color:magenta\">Select granularity level</li>                  <li style=\"color:magenta\">Set weights for validation metrics</li>                </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50676c383856473b838f50afe5339f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(SelectMultiple(description='File:', index=(0, 0), layout=Layout(height='200px', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service location: Local\n",
      "file source location: Local\n",
      "1 files submitted: CCGTD1_IPS_sample.zip\n",
      "file_types: ['zip']\n",
      "zip in file types\n",
      "experiment_id: 54\n",
      "min_cluster_size: 0\n",
      "Running clustering pipeline\n"
     ]
    }
   ],
   "source": [
    "# Dashboard\n",
    "run_button.on_click(run_service)\n",
    "display(HTML('<h1 style=\"color:magenta\">Cluster Activities</h1>\\\n",
    "              <p style=\"color:blue\">Use to following menus to submit a file for analysis:</p>\\\n",
    "                 <ul>\\\n",
    "                  <li style=\"color:magenta\">File to analyze</li>\\\n",
    "                  <li style=\"color:magenta\">Select granularity level</li>\\\n",
    "                  <li style=\"color:magenta\">Set weights for validation metrics</li>\\\n",
    "                </ul>'))\n",
    "data_files = [i for i in ([' '] + open('./tmp/files.txt').read().split('\\n')) if i]\n",
    "default = (data_files[0], ' ')\n",
    "file_dd=widgets.SelectMultiple(options=data_files,value=default,\n",
    "    description='File:',style=style,layout=features_layout)\n",
    "file_box = VBox(children=[file_dd, run_button])\n",
    "metrics_box = VBox(children=list(metrics_menus.values()))\n",
    "config_box = VBox(children=[apply_granularity, granularity, min_cluster_menu])\n",
    "HBox(children=[file_box, config_box, metrics_box])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19375cf90cbd4abfb0b803a26b95d125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Sort By (Desc):', layout=Layout(width='max-content'), options=('Alphabeti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def result_from_table(experiment_id, result_key='clusters'):\n",
    "    result_df = pd.read_sql_query(\"SELECT * FROM results \\\n",
    "    WHERE experiment_id={eid}\".format(eid=experiment_id), conn)\n",
    "    result = result_df['result'].values[0]\n",
    "    result = ast.literal_eval(result)\n",
    "    if result_key == 'clusters':    \n",
    "        result_key = [c for c in result.keys() if 'duration' not in c][0]\n",
    "    return result[result_key]\n",
    "\n",
    "\n",
    "def get_clusters(b):\n",
    "    experiment_id = experiments_dd.value\n",
    "    result_df = pd.read_sql_query(\"SELECT * FROM results \\\n",
    "    WHERE experiment_id={eid}\".format(eid=experiment_id), conn)\n",
    "    best_run_id = result_df['run_id'].values[0]\n",
    "    print('best run id=', best_run_id)\n",
    "    result = result_df['result'].values[0]\n",
    "    file_key = result_df['file_name'].values[0]\n",
    "    result = ast.literal_eval(result)\n",
    "    result = result[file_key]\n",
    "    #print('result:', result)\n",
    "    clusters_keys = list(result.keys())\n",
    "    sort_by = clusters_sort_dd.value\n",
    "    if sort_by == 'Alphabetic Order':\n",
    "        clusters_keys.sort()\n",
    "    else:\n",
    "        # Calculate %Overrun for sorting\n",
    "        planned_duration_dict = result_from_table(experiment_id, 'planned_duration_vals') \n",
    "        actual_duration_dict = result_from_table(experiment_id, 'actual_duration_vals') \n",
    "        clusters = result_from_table(experiment_id)\n",
    "        clusters_overruns = {}\n",
    "        for cluster_key in clusters_keys:\n",
    "            ids_names = clusters[cluster_key]\n",
    "            #print('ids_names:', ids_names)\n",
    "            cluster_ids = [i[0] for i in ids_names]\n",
    "            #print(cluster_ids)\n",
    "            cluster_planned_duration_dict = {k:v for k,v in planned_duration_dict.items() if k in cluster_ids}\n",
    "            cluster_actual_duration_dict = {k:v for k,v in actual_duration_dict.items() if k in cluster_ids}\n",
    "            #print('actual_duration_dict:', actual_duration_dict)\n",
    "            planned_in_actual_dict = {k:v for k,v in cluster_planned_duration_dict.items()\\\n",
    "                                      if k in cluster_actual_duration_dict.keys()}\n",
    "            tasks_overrun_perc = []\n",
    "            for id, task_planned_duration in planned_in_actual_dict.items():\n",
    "                task_actual_duration = actual_duration_dict[id]\n",
    "    #             print('id, task_actual_duration, task_planned_duration:', id,\\\n",
    "    #                   task_actual_duration, task_planned_duration)\n",
    "                if task_planned_duration != 0:\n",
    "                    task_overrun_perc = 100*((task_actual_duration/task_planned_duration)-1)\n",
    "    #                 if task_overrun_perc>1000:\n",
    "    #                     print('task:', id)\n",
    "    #                     print('task_overrun_perc:', task_overrun_perc)\n",
    "    #                     print('task_actual_duration, task_planned_duration:', task_actual_duration, task_planned_duration)\n",
    "                    tasks_overrun_perc.append(task_overrun_perc)\n",
    "            if tasks_overrun_perc:\n",
    "                mean_perc_overrun = round(int(np.mean(tasks_overrun_perc)), 2)\n",
    "            else: mean_perc_overrun = 0 \n",
    "            clusters_overruns[cluster_key] = mean_perc_overrun    \n",
    "        clusters_overruns = {k: v for k, v in sorted(clusters_overruns.items(),\\\n",
    "                                                     key=lambda item: item[1], reverse=True)}\n",
    "        #print('clusters_overruns:', clusters_overruns)\n",
    "        clusters_keys = list(clusters_overruns.keys())\n",
    "    clusters_keys = '\\n'.join(clusters_keys)\n",
    "    with open('./tmp/cluster_names.txt', 'w') as f: f.write(clusters_keys)\n",
    "\n",
    "#display(HTML('<h1 style=\"color:magenta\">Select Experiment</h1>'))\n",
    "service_location = service_button.value\n",
    "conn_params = location_db_params[service_location]\n",
    "conn = mysql.connect(**conn_params)\n",
    "experiment_ids = pd.read_sql_query(\"SELECT experiment_id FROM results\", conn).astype(int)\n",
    "experiment_ids = list(experiment_ids['experiment_id'].unique())\n",
    "if len(experiment_ids) == 0: experiment_ids = ['None']\n",
    "experiments_dd=widgets.Dropdown(options=experiment_ids, value=experiment_ids[0],\n",
    "    description='Select Experiment:',style=style, layout={'width': 'max-content'})\n",
    "sort_options = ['Alphabetic Order', 'Percent Overrun Mean Percentage']\n",
    "clusters_sort_dd=widgets.Dropdown(options=sort_options, value=sort_options[0],\n",
    "    description='Sort By (Desc):',style=style, layout={'width': 'max-content'})\n",
    "clusters_button = widgets.Button(description = \"Clusters \",style=style, layout={'width': 'max-content'})\n",
    "clusters_button.style.button_color = 'lightblue'\n",
    "clusters_button.on_click(get_clusters)\n",
    "HBox(children=[clusters_sort_dd, experiments_dd, clusters_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4428320da946bfb02d2b02780ac259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Select Cluster:', layout=Layout(width='max-content'), options=('None',), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_tasks(b):\n",
    "    cluster_key = clusters_keys_dd.value\n",
    "    print('Select cluster:', cluster_key)\n",
    "    experiment_id = experiments_dd.value\n",
    "    clusters = result_from_table(experiment_id)\n",
    "    ids_names = clusters[cluster_key]\n",
    "    ids_names = [' '.join(id_name) for id_name in ids_names]\n",
    "    ids_names = '\\n'.join(ids_names)\n",
    "    with open('./tmp/names.txt', 'w') as f: f.write(ids_names)\n",
    "        \n",
    "clusters_keys = open('./tmp/cluster_names.txt').read().split('\\n')\n",
    "clusters_keys_dd=widgets.Dropdown(options=clusters_keys, value=clusters_keys[0],\n",
    "    description='Select Cluster:',style=style, layout={'width': 'max-content'})\n",
    "tasks_button = widgets.Button(description = \"Tasks\",style=style, layout={'width': 'max-content'})\n",
    "tasks_button.style.button_color = 'orange'\n",
    "tasks_button.on_click(get_tasks)\n",
    "HBox(children=[clusters_keys_dd, experiments_dd, tasks_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad726ad29f1463ca800e76e03aa2b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Experiment:', layout=Layout(width='max-content'), options=(3,), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cluster_stats_plots(b):\n",
    "    cluster_key = clusters_keys_dd.value\n",
    "    experiment_id = experiments_dd.value\n",
    "    clusters = result_from_table(experiment_id)\n",
    "    names_to_exclude = ids_names_dd.value\n",
    "    exclude_ids = [i.split(' ')[0] for i in names_to_exclude]\n",
    "    \n",
    "    # Activities duration values\n",
    "    ids_names = {i[0]:i[1] for i in clusters[cluster_key] if i[0] not in exclude_ids}\n",
    "    ids, names = list(ids_names.keys()), list(ids_names.values())\n",
    "    new_key = get_cluster_key(names, cutoff=0.8)\n",
    "\n",
    "    planned_duration_dict = result_from_table(experiment_id, 'planned_duration_vals') \n",
    "    planned_duration_dict = {k:v for k,v in planned_duration_dict.items() if k in ids}\n",
    "    planned_duration_vals = list(planned_duration_dict.values())\n",
    "    \n",
    "    actual_duration_dict = result_from_table(experiment_id, 'actual_duration_vals') \n",
    "    actual_duration_dict = {k:v for k,v in actual_duration_dict.items() if k in ids}\n",
    "    actual_duration_vals = list(actual_duration_dict.values())\n",
    "    planned_in_actual_dict = {k:v for k,v in planned_duration_dict.items() if k in actual_duration_dict.keys()}\n",
    "    tasks_overrun_perc = []\n",
    "    \n",
    "    # Duration ratios and overruns\n",
    "    duration_ratios, tasks_overrun, tasks_overrun_perc = [], [], []\n",
    "    for id, task_planned_duration in planned_in_actual_dict.items():\n",
    "        task_actual_duration = actual_duration_dict[id]\n",
    "        if task_planned_duration != 0:\n",
    "            duration_ratios.append(round(task_actual_duration/task_planned_duration,2))\n",
    "            task_overrun = task_actual_duration-task_planned_duration\n",
    "            tasks_overrun.append(task_overrun)\n",
    "            tasks_overrun_perc.append(100*(task_actual_duration/task_planned_duration)-1)\n",
    "\n",
    "    ## Cluster Statistics\n",
    "    display(HTML('<h1 style=\"color:magenta\">Cluster RCF Analysis</h1>'))\n",
    "    print('New Key:', new_key)\n",
    "    print('Cluster Statistics')\n",
    "    print('Activities in Cluster:', len(ids))\n",
    "    print('Completed Activities in Cluster:', len(actual_duration_vals))\n",
    "    # Table\n",
    "    index = ['Planned Duration(Days)', 'Actual Duration(Days)', 'Overrun(Days)', 'Overrun(%)']\n",
    "    headers = ['MEAN', 'MEDIAN', 'STD']\n",
    "    def stats_row(arr): \n",
    "        if len(arr)>0:\n",
    "            return [np.mean(arr), np.median(arr), np.std(arr)]\n",
    "        else:\n",
    "            return(np.nan, np.nan, np.nan)\n",
    "    table_rows = [stats_row(planned_duration_vals), stats_row(actual_duration_vals),\\\n",
    "                 stats_row(tasks_overrun), stats_row(tasks_overrun_perc)]\n",
    "    stats_df = pd.DataFrame(table_rows, columns=headers, index=index)\n",
    "    stats_df = round(stats_df, 2)\n",
    "    #display(stats_df)                \n",
    "\n",
    "    # Plot Values: RCF for Tasks in Cluster\n",
    "    # x = percentile,  y = duration_ratios\n",
    "    duration_ratios.sort()\n",
    "    sum_ratios = sum(duration_ratios)\n",
    "    ratios_cumsum = np.cumsum(duration_ratios)\n",
    "    percentile = 100*(ratios_cumsum/sum_ratios)\n",
    "    rcf_df = pd.DataFrame(list(zip(percentile, duration_ratios)),\\\n",
    "                          columns = ['Percentile', 'Duration Ratio'])\n",
    "\n",
    "    # Plot Values: Duration Distribution\n",
    "    duration_type = len(planned_duration_vals) * ['Planned'] + len(actual_duration_vals) *['Actual']\n",
    "    duration_vals = list(planned_duration_vals)+list(actual_duration_vals)\n",
    "    duration_df = pd.DataFrame(list(zip(duration_type, duration_vals)), columns=['Duration', 'Days'])\n",
    "\n",
    "    # Cluster description\n",
    "    #print('planned_duration_vals:', planned_duration_vals)\n",
    "    #print('actual_duration_vals:', actual_duration_vals)\n",
    "    names_df = pd.DataFrame(ids, columns=['ID'])\n",
    "    names_df['Name'] = names\n",
    "    names_df['Planned Duration'] = planned_duration_vals\n",
    "    names_df['Actual Duration'] = actual_duration_vals\n",
    "\n",
    "    ## Display\n",
    "    # Tables\n",
    "    display_side_by_side(names_df,stats_df, titles=['Tasks in Cluster','Cluster Statistics'])\n",
    "\n",
    "    # Plots\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))#, sharey=True)\n",
    "    duration_dist = sns.boxplot(ax=axes[0], x=\"Duration\", y=\"Days\", data=duration_df)\n",
    "    rcf = sns.lineplot(ax=axes[1], x=\"Percentile\", y=\"Duration Ratio\", data=rcf_df)\n",
    "    axes[0].set_title('Duration Distibution (Tasks in Cluster)')\n",
    "    axes[1].set_title('RCF For Tasks in Cluster')\n",
    "\n",
    "\n",
    "ids_names = [' '] + open('./tmp/names.txt').read().split('\\n')\n",
    "layout={'width': 'max-content', 'height':'max-content'}\n",
    "ids_names_dd=widgets.SelectMultiple(options=ids_names, value=(ids_names[0], ' '),\n",
    "    description='Select names to exclude:',style=style, layout=layout)\n",
    "button = widgets.Button(description = \"Run RCF\",style=style)\n",
    "button.style.button_color = 'yellow'\n",
    "button.on_click(cluster_stats_plots)\n",
    "names_box = HBox(children=[ids_names_dd, button])\n",
    "VBox(children=[experiments_dd, names_box])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
